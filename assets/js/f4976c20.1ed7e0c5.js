"use strict";(self.webpackChunkroosevelt_advisors_website=self.webpackChunkroosevelt_advisors_website||[]).push([[2070],{28453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>o});var a=t(96540);const s={},r=a.createContext(s);function i(e){const n=a.useContext(r);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),a.createElement(r.Provider,{value:n},e.children)}},85686:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>a,toc:()=>c});var a=t(94534),s=t(74848),r=t(28453);const i={slug:"azure-ai-evaluation-apim-integration",title:"Building Reliable AI Agents - Implementing Advanced Evaluation with Azure AI SDK and Custom APIM Integration",authors:["jon"],tags:["azure","openai","evaluation","apim","llm","testing"],image:"/img/blog/2025-04-02-azure-ai-evaluation-apim/azure-eval-header.png"},o="Building Reliable AI Agents: Implementing Advanced Evaluation with Azure AI SDK and Custom APIM Integration",l={authorsImageUrls:[void 0]},c=[{value:"The Evaluation Challenge: Beyond Standard Setups",id:"the-evaluation-challenge-beyond-standard-setups",level:2},{value:"The APIM Roadblock",id:"the-apim-roadblock",level:2},{value:"Solution Part 1: Custom APIM-Aware Evaluators",id:"solution-part-1-custom-apim-aware-evaluators",level:2},{value:"Solution Part 2: The Manual Evaluation Loop",id:"solution-part-2-the-manual-evaluation-loop",level:2},{value:"Key Metrics in Practice",id:"key-metrics-in-practice",level:2},{value:"Groundedness Evaluation",id:"groundedness-evaluation",level:3},{value:"Relevance Evaluation",id:"relevance-evaluation",level:3},{value:"Implementation Insights",id:"implementation-insights",level:2},{value:"1. TypedDict for Response Typing",id:"1-typeddict-for-response-typing",level:3},{value:"2. Error Handling for API Stability",id:"2-error-handling-for-api-stability",level:3},{value:"3. Caching for Performance",id:"3-caching-for-performance",level:3},{value:"Lessons Learned and Recommendations",id:"lessons-learned-and-recommendations",level:2},{value:"For Microsoft",id:"for-microsoft",level:3},{value:"For Developers",id:"for-developers",level:3},{value:"Example Evaluation Results",id:"example-evaluation-results",level:2},{value:"Conclusion",id:"conclusion",level:2}];function u(e){const n={code:"code",h2:"h2",h3:"h3",img:"img",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"Learn how to implement robust evaluation for AI agents using Azure AI Evaluation SDK when working with Azure API Management (APIM), overcoming authentication and integration challenges."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Azure AI Evaluation with APIM",src:t(90251).A+"",width:"2752",height:"1536"})}),"\n","\n",(0,s.jsx)(n.p,{children:"In enterprise environments, evaluating the performance of AI agents is crucial for ensuring quality, reliability, and compliance. Microsoft's Azure AI Evaluation SDK provides powerful tools for this purpose\u2014but what happens when your AI services are accessed through Azure API Management (APIM) with custom authentication requirements? This post details our journey implementing a robust evaluation system for an AI research agent deployed behind APIM, sharing solutions to challenges that aren't covered in the standard documentation."}),"\n",(0,s.jsx)(n.h2,{id:"the-evaluation-challenge-beyond-standard-setups",children:"The Evaluation Challenge: Beyond Standard Setups"}),"\n",(0,s.jsx)(n.p,{children:"While Azure AI Evaluation SDK works seamlessly with direct Azure OpenAI endpoints, enterprise deployments often route API calls through Azure API Management (APIM) to:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Enforce Security Policies"}),": Custom authentication, rate limiting, IP filtering"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Add Business Logic"}),": Request transformation, logging, monitoring"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Manage Multiple Services"}),": Unified access point across various AI services"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This architectural choice creates significant challenges when trying to use standard evaluation tools:"}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TD\n    Client[Evaluation Client] --\x3e|Standard Path| Direct[Direct Azure OpenAI]\n    Client --\x3e|Enterprise Path| APIM[Azure API Management]\n    APIM --\x3e|Custom Auth Required| Direct\n    \n    style Enterprise Path stroke:#f66,stroke-width:2px\n    style APIM fill:#f66,stroke:#333,stroke-width:2px"}),"\n",(0,s.jsx)(n.p,{children:"Our research agent required evaluation across multiple metrics, including:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Groundedness (factual accuracy)"}),"\n",(0,s.jsx)(n.li,{children:"Answer relevancy"}),"\n",(0,s.jsx)(n.li,{children:"Contextual precision and recall"}),"\n",(0,s.jsx)(n.li,{children:"Faithfulness to sources"}),"\n",(0,s.jsx)(n.li,{children:"Fluency and coherence"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"the-apim-roadblock",children:"The APIM Roadblock"}),"\n",(0,s.jsx)(n.p,{children:"The standard evaluators from Azure AI Evaluation SDK failed when pointed at our APIM endpoint due to:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Authentication Failure"}),": Azure AD token authentication wasn't being properly passed through"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Missing Headers"}),": Required custom headers (",(0,s.jsx)(n.code,{children:"mkl-User-name"}),", ",(0,s.jsx)(n.code,{children:"username"}),") weren't included"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Incompatible Response Handling"}),": APIM response transformations broke evaluator expectations"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Here's what a typical failure looked like:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Standard approach - fails with APIM\nevaluators = {\n    "groundedness": GroundednessEvaluator(),\n    "relevance": RelevanceEvaluator()\n}\n\nresults = evaluate(\n    evaluators=evaluators,\n    target_function=run_research_agent,\n    test_cases=test_scenarios,\n    # Even when providing correct APIM URL, it fails with auth errors\n    azure_endpoint=os.getenv("AZURE_APIM_URI")\n)\n'})}),"\n",(0,s.jsxs)(n.p,{children:["This would consistently result in ",(0,s.jsx)(n.code,{children:"401 Unauthorized"})," or ",(0,s.jsx)(n.code,{children:"403 Forbidden"})," errors, even when environment variables for authentication were correctly set."]}),"\n",(0,s.jsx)(n.h2,{id:"solution-part-1-custom-apim-aware-evaluators",children:"Solution Part 1: Custom APIM-Aware Evaluators"}),"\n",(0,s.jsx)(n.p,{children:"Our first step was to create custom evaluator classes that could handle the APIM authentication and header requirements:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class APIMEvaluatorBase:\n    """Base class for custom APIM-compatible evaluators."""\n    \n    def __init__(self, metric_name, apim_config):\n        self._metric_name = metric_name\n        self.config = apim_config\n        \n        # Get OpenAI client with proper auth for APIM\n        self.client = self._create_apim_client()\n    \n    def _create_apim_client(self):\n        # Get Azure AD token\n        credential = DefaultAzureCredential()\n        token = credential.get_token("https://cognitiveservices.azure.com/.default")\n        \n        # Create client with custom headers for APIM\n        client = AzureOpenAI(\n            azure_endpoint=self.config.endpoint,\n            api_version=self.config.api_version,\n            api_key=token.token,  # Using token as API key\n        )\n        \n        # Add required APIM headers\n        headers = {\n            "mkl-User-name": self.config.username,\n            "username": self.config.username\n        }\n        \n        # Apply headers to all requests\n        client = client.with_additional_headers(headers)\n        return client\n        \n    def __call__(self, response, context=None, query=None):\n        """Evaluate with our APIM-aware client."""\n        # Implementation depends on specific metric\n        raise NotImplementedError()\n'})}),"\n",(0,s.jsx)(n.p,{children:"We then created specific subclasses for each evaluation metric:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class APIMGroundednessEvaluator(APIMEvaluatorBase):\n    """Evaluates groundedness using APIM-compatible client."""\n    \n    _metric_name = "groundedness"\n    \n    def __init__(self, apim_config):\n        super().__init__(self._metric_name, apim_config)\n    \n    def __call__(self, response, context=None, query=None):\n        """\n        Evaluates if the response is grounded in the provided context.\n        """\n        prompt = f"""\n        You are evaluating the factual accuracy of an AI assistant\'s response.\n        \n        Query: {query}\n        \n        Context (ground truth):\n        {context}\n        \n        Response to evaluate:\n        {response}\n        \n        Evaluate whether the response contains only information that is present in the context.\n        \n        Score from 0-5, where:\n        0: Completely ungrounded, containing major hallucinations\n        1: Mostly ungrounded with several inaccuracies\n        2: Partially grounded, but with notable inaccuracies\n        3: Mostly grounded with minor inaccuracies\n        4: Almost completely grounded, with very minor inaccuracies\n        5: Completely grounded, only containing information from the context\n        \n        Format your response as a JSON object with the following keys:\n        - score: The numerical score (0-5)\n        - reasoning: Your step-by-step evaluation\n        """\n        \n        try:\n            # Call Azure OpenAI via APIM with our authenticated client\n            response = self.client.chat.completions.create(\n                model=self.config.deployment_name,\n                messages=[{"role": "user", "content": prompt}],\n                temperature=0.0,\n                response_format={"type": "json_object"}\n            )\n            \n            # Parse JSON response\n            content = response.choices[0].message.content\n            result = json.loads(content)\n            \n            # Return properly formatted result\n            return {\n                "score": result["score"],\n                "reasoning": result["reasoning"]\n            }\n            \n        except Exception as e:\n            logger.error(f"Error evaluating groundedness: {e}")\n            return {"score": 0, "reasoning": f"Evaluation failed: {str(e)}"}\n'})}),"\n",(0,s.jsxs)(n.p,{children:["We implemented similar classes for the other metrics: ",(0,s.jsx)(n.code,{children:"APIMRelevanceEvaluator"}),", ",(0,s.jsx)(n.code,{children:"APIMContextualPrecisionEvaluator"}),", etc."]}),"\n",(0,s.jsx)(n.h2,{id:"solution-part-2-the-manual-evaluation-loop",children:"Solution Part 2: The Manual Evaluation Loop"}),"\n",(0,s.jsxs)(n.p,{children:["While our custom evaluators solved the authentication and header issues, we encountered an unexpected problem when trying to use them with the SDK's ",(0,s.jsx)(n.code,{children:"evaluate()"})," function. Even though our individual evaluators worked correctly, the evaluation framework couldn't properly aggregate their results."]}),"\n",(0,s.jsxs)(n.p,{children:["After extensive debugging, we found that the SDK's ",(0,s.jsx)(n.code,{children:"evaluate()"})," function has limitations when handling custom evaluators that use direct API calls with custom authentication. The inner workings of the function assume certain behavior that our APIM-aware evaluators couldn't satisfy."]}),"\n",(0,s.jsx)(n.p,{children:"Our solution was to implement a manual evaluation loop:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def run_manual_evaluation(test_scenarios, apim_config):\n    """Run evaluation manually with custom APIM-aware evaluators."""\n    \n    # Initialize our custom evaluators\n    evaluators = {\n        "groundedness": APIMGroundednessEvaluator(apim_config),\n        "relevance": APIMRelevanceEvaluator(apim_config),\n        "contextual_precision": APIMContextualPrecisionEvaluator(apim_config),\n        "faithfulness": APIMFaithfulnessEvaluator(apim_config),\n        "fluency": APIMFluencyEvaluator(apim_config)\n    }\n    \n    # Define threshold for each metric\n    thresholds = {\n        "groundedness": 3.5,\n        "relevance": 3.5,\n        "contextual_precision": 3.0,\n        "faithfulness": 3.5,\n        "fluency": 3.0\n    }\n    \n    # Store results for each scenario and metric\n    all_results = []\n    \n    # Process each test scenario\n    for scenario in test_scenarios:\n        # Run the agent to get a response\n        agent_response = run_research_agent(\n            query=scenario["query"],\n            additional_context=scenario.get("additional_context", "")\n        )\n        \n        # Evaluate with each metric\n        scenario_results = {\n            "query": scenario["query"],\n            "response": agent_response,\n            "metrics": {}\n        }\n        \n        for metric_name, evaluator in evaluators.items():\n            # Skip metrics not required for this scenario\n            if metric_name not in scenario.get("evaluation_metrics", list(evaluators.keys())):\n                continue\n                \n            # Run evaluation\n            result = evaluator(\n                response=agent_response,\n                context=scenario.get("context", ""),\n                query=scenario["query"]\n            )\n            \n            # Store result\n            scenario_results["metrics"][metric_name] = {\n                "score": result["score"],\n                "reasoning": result["reasoning"],\n                "threshold": thresholds[metric_name],\n                "pass": result["score"] >= thresholds[metric_name]\n            }\n        \n        all_results.append(scenario_results)\n    \n    # Calculate overall metrics\n    summary = calculate_evaluation_summary(all_results, thresholds)\n    \n    # Save results to file\n    save_evaluation_results(all_results, summary)\n    \n    return all_results, summary\n'})}),"\n",(0,s.jsx)(n.p,{children:"This manual approach gave us complete control over the evaluation process, while still using the custom evaluators we created to interface with APIM."}),"\n",(0,s.jsx)(n.h2,{id:"key-metrics-in-practice",children:"Key Metrics in Practice"}),"\n",(0,s.jsx)(n.p,{children:"Let's look at how some of the key metrics worked in real-world evaluation of our research agent:"}),"\n",(0,s.jsx)(n.h3,{id:"groundedness-evaluation",children:"Groundedness Evaluation"}),"\n",(0,s.jsx)(n.p,{children:"Groundedness measures whether the AI's response contains information that's supported by the provided context:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Example scenario with groundedness evaluation\nscenario = {\n    "query": "What are the main environmental risks for offshore drilling?",\n    "context": "Offshore drilling poses several environmental risks including: \n               oil spills that can damage marine ecosystems, \n               disruption of marine habitats during drilling, \n               underwater noise pollution affecting marine mammals, \n               and greenhouse gas emissions from flaring operations.",\n    "evaluation_metrics": ["groundedness", "relevance"]\n}\n\n# Agent response\nresponse = "Offshore drilling presents significant environmental concerns. \n            The primary risks include catastrophic oil spills that damage marine ecosystems,\n            physical disruption of seafloor habitats during drilling operations,\n            noise pollution that can disorient and harm marine mammals like whales and dolphins,\n            and substantial greenhouse gas emissions from gas flaring."\n\n# Evaluator assessment (simplified)\nresult = {\n    "score": 5.0,  # Perfect score - all information is grounded\n    "reasoning": "The response accurately reflects all environmental risks mentioned \n                 in the context without adding unsupported claims."\n}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"relevance-evaluation",children:"Relevance Evaluation"}),"\n",(0,s.jsx)(n.p,{children:"Relevance assesses how well the response addresses the specific query:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Example with lower relevance score\nscenario = {\n    "query": "What regulations govern offshore drilling safety?",\n    "context": "Offshore drilling is regulated by several frameworks including \n               the BSEE regulations in the US, which mandate safety equipment like blowout preventers. \n               The SEMS rule requires safety management systems. \n               International operations often follow IMO guidelines and regional regulations."\n}\n\n# Less relevant response\nresponse = "Offshore drilling has strict regulations. Companies must follow \n            environmental protection standards and obtain permits before drilling. \n            Environmental impact assessments are required in many jurisdictions."\n\n# Evaluator assessment\nresult = {\n    "score": 2.0,  # Low score - response doesn\'t address safety regulations specifically\n    "reasoning": "The response discusses regulations but focuses on environmental permits\n                 rather than the safety regulations mentioned in the context. \n                 It doesn\'t address BSEE, SEMS, or IMO guidelines specifically asked about in the query."\n}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"implementation-insights",children:"Implementation Insights"}),"\n",(0,s.jsx)(n.p,{children:"During our implementation, we discovered several important insights:"}),"\n",(0,s.jsx)(n.h3,{id:"1-typeddict-for-response-typing",children:"1. TypedDict for Response Typing"}),"\n",(0,s.jsxs)(n.p,{children:["Azure AI Evaluation expects specific return types from evaluator ",(0,s.jsx)(n.code,{children:"__call__"})," methods. Using Python's ",(0,s.jsx)(n.code,{children:"TypedDict"})," ensures compatibility:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from typing import TypedDict, List, Dict\n\nclass GroundednessResult(TypedDict):\n    score: float\n    reasoning: str\n\nclass APIMGroundednessEvaluator(APIMEvaluatorBase):\n    # ...\n    def __call__(self, response, context=None, query=None) -> GroundednessResult:\n        # Implementation\n        # ...\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-error-handling-for-api-stability",children:"2. Error Handling for API Stability"}),"\n",(0,s.jsx)(n.p,{children:"Our APIM calls occasionally encountered timeout or transient errors. Adding robust error handling improved stability:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def safe_api_call(client, *args, max_retries=3, **kwargs):\n    """Make API call with retry logic."""\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(*args, **kwargs)\n        except (APITimeoutError, ServiceUnavailableError) as e:\n            if attempt == max_retries - 1:\n                raise\n            time.sleep(2 ** attempt)  # Exponential backoff\n'})}),"\n",(0,s.jsx)(n.h3,{id:"3-caching-for-performance",children:"3. Caching for Performance"}),"\n",(0,s.jsx)(n.p,{children:"Evaluation can be time-consuming and expensive. We implemented caching to avoid redundant API calls:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class CachingEvaluator:\n    """Wrapper for evaluators that caches results."""\n    \n    def __init__(self, evaluator, cache_file=None):\n        self.evaluator = evaluator\n        self.cache_file = cache_file or f"{type(evaluator).__name__}_cache.json"\n        self.cache = self._load_cache()\n    \n    def _load_cache(self):\n        if os.path.exists(self.cache_file):\n            with open(self.cache_file, \'r\') as f:\n                return json.load(f)\n        return {}\n    \n    def _save_cache(self):\n        with open(self.cache_file, \'w\') as f:\n            json.dump(self.cache, f)\n    \n    def __call__(self, response, context=None, query=None):\n        # Create cache key from inputs\n        key = hashlib.md5(f"{query}|{context}|{response}".encode()).hexdigest()\n        \n        if key in self.cache:\n            return self.cache[key]\n        \n        # Call underlying evaluator\n        result = self.evaluator(response, context, query)\n        \n        # Cache result\n        self.cache[key] = result\n        self._save_cache()\n        \n        return result\n'})}),"\n",(0,s.jsx)(n.h2,{id:"lessons-learned-and-recommendations",children:"Lessons Learned and Recommendations"}),"\n",(0,s.jsx)(n.p,{children:"Based on our experience, here are key recommendations for implementing evaluation with APIM:"}),"\n",(0,s.jsx)(n.h3,{id:"for-microsoft",children:"For Microsoft"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Better APIM Documentation"}),": Add specific guidance for using evaluation tools with APIM-protected endpoints."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Authentication Flexibility"}),": Enhance the SDK to support Azure AD token auth and custom headers."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Extension Points"}),": Provide clearer hooks for extending evaluators with custom authentication."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"for-developers",children:"For Developers"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Start Simple"}),": Test individual evaluator classes before attempting to use the full evaluate() framework."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Error Logging"}),": Implement verbose logging for evaluation failures to identify auth/header issues."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Manual Control"}),": Don't hesitate to implement your own evaluation loop for complex scenarios."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Test Environment"}),": Create a test environment without APIM to validate evaluation logic before adding APIM complexity."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"example-evaluation-results",children:"Example Evaluation Results"}),"\n",(0,s.jsx)(n.p,{children:"We ran our custom evaluation system against a research agent tasked with investigating various companies. Here's a sample of the results:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n  "summary": {\n    "groundedness": {\n      "average_score": 4.7,\n      "pass_rate": 0.95,\n      "threshold": 3.5\n    },\n    "relevance": {\n      "average_score": 4.5,\n      "pass_rate": 0.92,\n      "threshold": 3.5\n    },\n    "contextual_precision": {\n      "average_score": 4.2,\n      "pass_rate": 0.89,\n      "threshold": 3.0\n    },\n    "faithfulness": {\n      "average_score": 4.6,\n      "pass_rate": 0.94,\n      "threshold": 3.5\n    },\n    "fluency": {\n      "average_score": 4.8,\n      "pass_rate": 0.98,\n      "threshold": 3.0\n    },\n    "overall_pass_rate": 0.91\n  }\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:"These results helped identify weaknesses in our agent and guided improvements to the prompts, retrieval strategies, and overall system design."}),"\n",(0,s.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(n.p,{children:"Implementing advanced evaluation for AI agents in an enterprise environment with APIM presents unique challenges, but the effort is worthwhile. Our custom APIM-aware evaluators and manual evaluation loop provided comprehensive quality assessment capabilities while working within the constraints of our enterprise architecture."}),"\n",(0,s.jsx)(n.p,{children:"By sharing our approach, challenges, and solutions, we hope to help others implement robust evaluation systems for their AI agents, even when the standard tools don't fit perfectly with their enterprise architecture."}),"\n",(0,s.jsx)(n.p,{children:"The quality of AI systems depends not just on their capabilities, but on our ability to reliably measure their performance. With the right evaluation approach, we can build AI agents that are not only powerful but trustworthy and reliable in enterprise contexts."})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}},90251:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/azure-eval-header-4e62e43c21a3dc455f6b62ad56034085.png"},94534:e=>{e.exports=JSON.parse('{"permalink":"/blog/azure-ai-evaluation-apim-integration","source":"@site/blog/2025-04-02-azure-ai-evaluation-apim/index.mdx","title":"Building Reliable AI Agents - Implementing Advanced Evaluation with Azure AI SDK and Custom APIM Integration","description":"Learn how to implement robust evaluation for AI agents using Azure AI Evaluation SDK when working with Azure API Management (APIM), overcoming authentication and integration challenges.","date":"2025-04-02T00:00:00.000Z","tags":[{"inline":false,"label":"Azure","permalink":"/blog/tags/tags/azure","description":"Microsoft Azure cloud platform and services"},{"inline":false,"label":"OpenAI","permalink":"/blog/tags/tags/openai","description":"OpenAI models and technologies"},{"inline":false,"label":"Evaluation","permalink":"/blog/tags/tags/evaluation","description":"Model and system evaluation techniques"},{"inline":false,"label":"APIM","permalink":"/blog/tags/tags/apim","description":"API Management solutions and practices"},{"inline":false,"label":"LLM","permalink":"/blog/tags/tags/llm","description":"Large Language Models and their applications"},{"inline":false,"label":"Testing","permalink":"/blog/tags/tags/testing","description":"Testing methodologies and approaches"}],"readingTime":9.575,"hasTruncateMarker":true,"authors":[{"name":"Jon Roosevelt","title":"Consultant","url":"https://jonroosevelt.com","page":{"permalink":"/blog/authors/all-jon-roosevelt-articles"},"socials":{"linkedin":"https://www.linkedin.com/in/jonroosevelt/","github":"https://github.com/RooseveltAdvisors"},"imageURL":"https://github.com/RooseveltAdvisors.png","key":"jon"}],"frontMatter":{"slug":"azure-ai-evaluation-apim-integration","title":"Building Reliable AI Agents - Implementing Advanced Evaluation with Azure AI SDK and Custom APIM Integration","authors":["jon"],"tags":["azure","openai","evaluation","apim","llm","testing"],"image":"/img/blog/2025-04-02-azure-ai-evaluation-apim/azure-eval-header.png"},"unlisted":false,"prevItem":{"title":"Architecting Extensible AI Agents - A Modular Core with Pluggable Skills and SSE Communication","permalink":"/blog/architecting-modular-ai-agents"},"nextItem":{"title":"Porting GPTResearcher to Semantic Kernel - Building an Enterprise-Ready Research Agent","permalink":"/blog/semantic-kernel-research-agent"}}')}}]);
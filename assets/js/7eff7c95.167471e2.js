"use strict";(self.webpackChunkroosevelt_advisors_website=self.webpackChunkroosevelt_advisors_website||[]).push([[1315],{6492:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>s,metadata:()=>t,toc:()=>d});var t=i(31397),a=i(74848),o=i(28453);const s={slug:"fine-tuning-microsoft-phi-2-for-sentiment-analysis",title:"Fine-Tuning Microsoft Phi-2 for Sentiment Analysis - A Step-by-Step Guide",authors:["jon"],tags:["llm","fine-tuning","sentiment-analysis","microsoft","phi-2","machine-learning","nlp"],image:"/img/blog/fine-tuning-microsoft-phi-2-for-sentiment-analysis-a-step-by-step-guide/header.jpg"},r="Fine-Tuning Microsoft Phi-2 for Sentiment Analysis: A Step-by-Step Guide",l={authorsImageUrls:[void 0]},d=[{value:"Introduction",id:"introduction",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"1. Data Preparation",id:"1-data-preparation",level:2},{value:"Loading and Processing the Data",id:"loading-and-processing-the-data",level:3},{value:"Creating Balanced Datasets",id:"creating-balanced-datasets",level:3},{value:"2. Model Setup and Quantization",id:"2-model-setup-and-quantization",level:2},{value:"Configuring the Model",id:"configuring-the-model",level:3},{value:"3. Fine-Tuning with LoRA",id:"3-fine-tuning-with-lora",level:2},{value:"Setting up LoRA Configuration",id:"setting-up-lora-configuration",level:3},{value:"Training Configuration",id:"training-configuration",level:3},{value:"4. Model Training and Evaluation",id:"4-model-training-and-evaluation",level:2},{value:"Training the Model",id:"training-the-model",level:3},{value:"Evaluation Metrics",id:"evaluation-metrics",level:3},{value:"5. Saving and Deploying the Model",id:"5-saving-and-deploying-the-model",level:2},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.p,{children:"Microsoft Phi-2 Fine Tuning - Learn how to adapt this powerful small language model for sentiment analysis of employee performance data using LoRA and quantization."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Microsoft Phi-2 Fine Tuning",src:i(13329).A+"",width:"2752",height:"1536"})}),"\n","\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"In this comprehensive guide, we'll walk through the process of fine-tuning Microsoft's Phi-2 model for sentiment analysis of employee performance data. We'll cover everything from data preparation to model evaluation, using advanced techniques like LoRA (Low-Rank Adaptation) and quantization."}),"\n",(0,a.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsx)(n.p,{children:"Before we begin, ensure you have the following dependencies installed:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"pip install accelerate peft einops datasets bitsandbytes trl transformers datasets\n"})}),"\n",(0,a.jsx)(n.h2,{id:"1-data-preparation",children:"1. Data Preparation"}),"\n",(0,a.jsx)(n.h3,{id:"loading-and-processing-the-data",children:"Loading and Processing the Data"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load data\nfilename = "./data/teacher_performance/ReadyToTrain_data_2col_with_subjectivity_final.tsv"\ntraining_data_df = pd.read_csv(\n    filename,\n    sep=\'\\t\',\n    encoding="utf-8",\n    encoding_errors="replace"\n)\n\n# Filter comments longer than 200 characters\ntraining_data_df = training_data_df[\n    training_data_df[\'StudentComments\'].str.len() > 200\n]\n'})}),"\n",(0,a.jsx)(n.h3,{id:"creating-balanced-datasets",children:"Creating Balanced Datasets"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Split data for each sentiment\nX_train = []\nX_test = []\n\nfor sentiment in ['positive', 'neutral', 'negative']:\n    train, test = train_test_split(\n        training_data_df[training_data_df['Sentiment'] == sentiment],\n        random_state=42\n    )\n    X_train.append(train)\n    X_test.append(test)\n"})}),"\n",(0,a.jsx)(n.h2,{id:"2-model-setup-and-quantization",children:"2. Model Setup and Quantization"}),"\n",(0,a.jsx)(n.h3,{id:"configuring-the-model",children:"Configuring the Model"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig\n)\n\n# Quantization configuration\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type="nf4",\n    bnb_4bit_compute_dtype=torch.float16\n)\n\n# Load model and tokenizer\nbase_model = AutoModelForCausalLM.from_pretrained(\n    "microsoft/phi-2",\n    trust_remote_code=True,\n    device_map="auto",\n    quantization_config=bnb_config\n)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"3-fine-tuning-with-lora",children:"3. Fine-Tuning with LoRA"}),"\n",(0,a.jsx)(n.h3,{id:"setting-up-lora-configuration",children:"Setting up LoRA Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from peft import LoraConfig\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=16,\n    target_modules=[\n        "q_proj",\n        "k_proj",\n        "v_proj",\n        "dense"\n    ],\n    lora_dropout=0.05,\n    bias="none",\n    task_type="CAUSAL_LM"\n)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"training-configuration",children:"Training Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'training_arguments = TrainingArguments(\n    output_dir="./runs/Sentiment-Analysis-Phi2-fine-tuned",\n    num_train_epochs=50,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=8,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=True\n)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"4-model-training-and-evaluation",children:"4. Model Training and Evaluation"}),"\n",(0,a.jsx)(n.h3,{id:"training-the-model",children:"Training the Model"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"sft_trainer = SFTTrainer(\n    model=base_model,\n    train_dataset=train_data,\n    eval_dataset=eval_data,\n    peft_config=lora_config,\n    tokenizer=tokenizer,\n    args=training_arguments\n)\n\nsft_trainer.train()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,a.jsx)(n.p,{children:"The model showed significant improvements after fine-tuning:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Overall accuracy increased from 34.9% to 87.2%"}),"\n",(0,a.jsx)(n.li,{children:"Positive sentiment accuracy improved from 7.3% to 97.0%"}),"\n",(0,a.jsx)(n.li,{children:"Negative sentiment accuracy increased from 11.0% to 84.7%"}),"\n",(0,a.jsx)(n.li,{children:"Training loss reduced by 38% (from 1.41 to 0.87)"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"5-saving-and-deploying-the-model",children:"5. Saving and Deploying the Model"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# This code block is used to load the trained model, merge it, and save the merged model.\nmerged_model_path = f\"{base_dir}/merged_model\"\n# 'AutoPeftModelForCausalLM' is a class from the 'peft' library that provides a causal language model with PEFT (Performance Efficient Fine-Tuning) support.\n\nfrom peft import AutoPeftModelForCausalLM\n\n# 'AutoPeftModelForCausalLM.from_pretrained' is a method that loads a pre-trained model (adapter model) and its base model.\n#  The adapter model is loaded from 'args.output_dir', which is the directory where the trained model was saved.\n# 'low_cpu_mem_usage' is set to True, which means that the model will use less CPU memory.\n# 'return_dict' is set to True, which means that the model will return a 'ModelOutput' (a named tuple) instead of a plain tuple.\n# 'torch_dtype' is set to 'torch.bfloat16', which means that the model will use bfloat16 precision for its computations.\n# 'trust_remote_code' is set to True, which means that the model will trust and execute remote code.\n# 'device_map' is the device map that will be used by the model.\n\n# 'device_map' is a dictionary that maps the model to the GPU device.\n# In this case, the entire model is loaded on GPU 0.\ndevice_map = {\"\": 0}\nnew_model = AutoPeftModelForCausalLM.from_pretrained(\n    base_dir,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.bfloat16, #torch.float16,\n    trust_remote_code=True,\n    device_map=device_map,\n)\n\n# 'new_model.merge_and_unload' is a method that merges the model and unloads it from memory.\n# The merged model is stored in 'merged_model'.\n\nmerged_model = new_model.merge_and_unload()\n\n# 'merged_model.save_pretrained' is a method that saves the merged model.\n# The model is saved in the directory \"merged_model\".\n# 'trust_remote_code' is set to True, which means that the model will trust and execute remote code.\n# 'safe_serialization' is set to True, which means that the model will use safe serialization.\n\nmerged_model.save_pretrained(merged_model_path, trust_remote_code=True, safe_serialization=True)\n\n# 'tokenizer.save_pretrained' is a method that saves the tokenizer.\n# The tokenizer is saved in the directory \"merged_model\".\n\ntokenizer.save_pretrained(merged_model_path)\n"})}),"\n",(0,a.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsx)(n.p,{children:"Through this fine-tuning process, I've successfully adapted the Microsoft Phi-2 model for sentiment analysis, achieving significant improvements in accuracy across all sentiment categories. The use of LoRA and quantization techniques helped maintain efficiency while improving performance."}),"\n",(0,a.jsxs)(n.p,{children:["The full training code is available on my ",(0,a.jsx)(n.a,{href:"https://github.com/RooseveltAdvisors/jr_playground/blob/main/Phi2%20Fine%20Tuning%20for%20Sentiment%20Analysis.ipynb",children:"GitHub repository"}),". If you have any questions, please don't hesitate to email me at ",(0,a.jsx)(n.a,{href:"mailto:rooseveltadvisors@gmail.com",children:"rooseveltadvisors@gmail.com"}),"."]})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},13329:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/header-af628601d819567cf6e7b0761aa43e95.jpg"},28453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>r});var t=i(96540);const a={},o=t.createContext(a);function s(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),t.createElement(o.Provider,{value:n},e.children)}},31397:e=>{e.exports=JSON.parse('{"permalink":"/blog/fine-tuning-microsoft-phi-2-for-sentiment-analysis","source":"@site/blog/2024-11-15-fine-tuning-microsoft-phi-2-for-sentiment-analysis-a-step-by-step-guide/index.mdx","title":"Fine-Tuning Microsoft Phi-2 for Sentiment Analysis - A Step-by-Step Guide","description":"Microsoft Phi-2 Fine Tuning - Learn how to adapt this powerful small language model for sentiment analysis of employee performance data using LoRA and quantization.","date":"2024-11-15T00:00:00.000Z","tags":[{"inline":false,"label":"LLM","permalink":"/blog/tags/tags/llm","description":"Large Language Models and their applications"},{"inline":false,"label":"Fine-tuning","permalink":"/blog/tags/tags/fine-tuning","description":"Articles about fine-tuning and adapting language models"},{"inline":false,"label":"Sentiment Analysis","permalink":"/blog/tags/tags/sentiment-analysis","description":"Content about analyzing and classifying sentiment in text"},{"inline":false,"label":"Microsoft","permalink":"/blog/tags/tags/microsoft","description":"Content related to Microsoft technologies and services"},{"inline":false,"label":"Phi-2","permalink":"/blog/tags/tags/phi-2","description":"Content related to Microsoft\'s Phi-2 small language model"},{"inline":false,"label":"Machine Learning","permalink":"/blog/tags/tags/machine-learning","description":"Topics covering machine learning algorithms and applications"},{"inline":false,"label":"NLP","permalink":"/blog/tags/tags/nlp","description":"Natural Language Processing techniques and applications"}],"readingTime":3.625,"hasTruncateMarker":true,"authors":[{"name":"Jon Roosevelt","title":"Consultant","url":"https://jonroosevelt.com","page":{"permalink":"/blog/authors/all-jon-roosevelt-articles"},"socials":{"linkedin":"https://www.linkedin.com/in/jonroosevelt/","github":"https://github.com/RooseveltAdvisors"},"imageURL":"https://github.com/RooseveltAdvisors.png","key":"jon"}],"frontMatter":{"slug":"fine-tuning-microsoft-phi-2-for-sentiment-analysis","title":"Fine-Tuning Microsoft Phi-2 for Sentiment Analysis - A Step-by-Step Guide","authors":["jon"],"tags":["llm","fine-tuning","sentiment-analysis","microsoft","phi-2","machine-learning","nlp"],"image":"/img/blog/fine-tuning-microsoft-phi-2-for-sentiment-analysis-a-step-by-step-guide/header.jpg"},"unlisted":false,"prevItem":{"title":"Defining PII Masking Policies with AWS Bedrock Guardrails","permalink":"/blog/2024/11/26/defining-pii-masking-policies-with-aws-bedrock-guardrails"},"nextItem":{"title":"Optimizing Apache Spark Performance for Skewed Data - Advanced Techniques and Case Study","permalink":"/blog/handling-skewed-data-in-apache-spark-performance-optimization"}}')}}]);